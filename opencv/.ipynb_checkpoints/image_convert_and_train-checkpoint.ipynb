{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamamotomasaomi/.pyenv/versions/anaconda3-5.1.0/envs/makuwo/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "PATH_AND_LABEL = []\n",
    "\n",
    "with open (\"path_and_label\", mode=\"r\") as file :\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        line_list = line.split()\n",
    "        PATH_AND_LABEL.append(line_list)\n",
    "        random.shuffle(PATH_AND_LABEL)\n",
    "\n",
    "DATA_SET = []\n",
    "\n",
    "for path_label in PATH_AND_LABEL :\n",
    "    tmp_list = []\n",
    "    \n",
    "    img = cv.imread(path_label[0])\n",
    "    img = cv.resize(img,(28,28))\n",
    "    \n",
    "    img = img.flatten().astype(np.float32)/255.0\n",
    "    \n",
    "    tmp_list.append(img)\n",
    "    \n",
    "    classes_array = np.zeros(3, dtype = 'float64')\n",
    "    if n >= 104:\n",
    "     classes_array[int(path_label[0])] = 1\n",
    "    else :\n",
    "        classes_array[int(path_label[1])] = 1\n",
    "    \n",
    "    tmp_list.append(classes_array)\n",
    "    DATA_SET.append(tmp_list)\n",
    "\n",
    "TRAIN_DATA_SIZE = int(len(DATA_SET) * 0.8)\n",
    "TRAIN_DATA_SET = DATA_SET[:TRAIN_DATA_SIZE]\n",
    "TEST_DATA_SET = DATA_SET[TRAIN_DATA_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 3\n",
    "NUM_CLASSES = 3\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_MATRIX_SIZE = IMAGE_SIZE * IMAGE_SIZE * CHANNELS\n",
    "PATH_LABEL_FILE = \"path_and_label\"\n",
    "\n",
    "def batch_data(data_set, batch_size) :\n",
    "\n",
    "    data_set = random.sample(data_set, batch_size)\n",
    "\n",
    "    return data_set\n",
    "def devide_data_set(data_set) :\n",
    "    # ndarrayにすることで、以下のように配列にアクセスすることができる。\n",
    "    data_set = np.array(data_set)\n",
    "    image_data_set = data_set[:int(len(data_set)), :1].flatten()\n",
    "    label_data_set = data_set[:int(len(data_set)), 1:].flatten()\n",
    "\n",
    "    image_ndarray = np.empty((0, 2352))\n",
    "    label_ndarray = np.empty((0, 3))\n",
    "\n",
    "    for (img, label) in zip(image_data_set, label_data_set) :\n",
    "        image_ndarray = np.append(image_ndarray, np.reshape(img, (1, 2352)), axis=0)\n",
    "        label_ndarray = np.append(label_ndarray, np.reshape(label, (1, 32)), axis=0)\n",
    "\n",
    "    return image_ndarray, label_ndarray\n",
    "def batch_data(data_set, batch_size) :\n",
    "\n",
    "    data_set = random.sample(data_set, batch_size)\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def devide_data_set(data_set) :\n",
    "    data_set = np.array(data_set)\n",
    "    image_data_set = data_set[:int(len(data_set)), :1].flatten()\n",
    "    label_data_set = data_set[:int(len(data_set)), 1:].flatten()\n",
    "\n",
    "    image_ndarray = np.empty((0, IMAGE_MATRIX_SIZE))\n",
    "    label_ndarray = np.empty((0, NUM_CLASSES))\n",
    "\n",
    "    for (img, label) in zip(image_data_set, label_data_set) :\n",
    "        image_ndarray = np.append(image_ndarray, np.reshape(img, (1, IMAGE_MATRIX_SIZE)), axis=0)\n",
    "        label_ndarray = np.append(label_ndarray, np.reshape(label, (1, NUM_CLASSES)), axis=0)\n",
    "\n",
    "    return image_ndarray, label_ndarray\n",
    "\n",
    "\n",
    "def conv2d(x, W) :\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x) :\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "def weight_variable(shape) :\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape) :\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def deepnn(x) :\n",
    "\n",
    "    with tf.name_scope('reshape') :\n",
    "        x_image = tf.reshape(x, [-1, IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n",
    "\n",
    "    with tf.name_scope('conv1') :\n",
    "        W_conv1 = weight_variable([5, 5, CHANNELS, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    with tf.name_scope('pool1') :\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    with tf.name_scope('conv2') :\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    with tf.name_scope('pool2') :\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "    with tf.name_scope('fc1') :\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    with tf.name_scope('dropout') :\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    with tf.name_scope('fc2') :\n",
    "        W_fc2 = weight_variable([1024, NUM_CLASSES])\n",
    "        b_fc2 = bias_variable([NUM_CLASSES])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    return y_conv, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-770bdadd7abd>:99: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "epoch_step 0, training accuracy 0.92\n",
      "epoch_step 50, training accuracy 1\n",
      "test accuracy 0.869565\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-770bdadd7abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;31m# epoch_step ~, training accuracy ~\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# test accuracy ~\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-770bdadd7abd>\u001b[0m in \u001b[0;36mview\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMAGE_MATRIX_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_' is not defined"
     ]
    }
   ],
   "source": [
    "def devide_data_set(data_set) :\n",
    "    # ndarrayにすることで、以下のように配列にアクセスすることができる。\n",
    "    data_set = np.array(data_set)\n",
    "    image_data_set = data_set[:int(len(data_set)), :1].flatten()\n",
    "    label_data_set = data_set[:int(len(data_set)), 1:].flatten()\n",
    "\n",
    "    image_ndarray = np.empty((0, 2352))\n",
    "    label_ndarray = np.empty((0, 3))\n",
    "\n",
    "    for (img, label) in zip(image_data_set, label_data_set) :\n",
    "        image_ndarray = np.append(image_ndarray, np.reshape(img, (1, 2352)), axis=0)\n",
    "        label_ndarray = np.append(label_ndarray, np.reshape(label, (1, 3)), axis=0)\n",
    "\n",
    "    return image_ndarray, label_ndarray\n",
    "def batch_data(data_set, batch_size) :\n",
    "\n",
    "    data_set = random.sample(data_set, batch_size)\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def devide_data_set(data_set) :\n",
    "    data_set = np.array(data_set)\n",
    "    image_data_set = data_set[:int(len(data_set)), :1].flatten()\n",
    "    label_data_set = data_set[:int(len(data_set)), 1:].flatten()\n",
    "\n",
    "    image_ndarray = np.empty((0, IMAGE_MATRIX_SIZE))\n",
    "    label_ndarray = np.empty((0, NUM_CLASSES))\n",
    "\n",
    "    for (img, label) in zip(image_data_set, label_data_set) :\n",
    "        image_ndarray = np.append(image_ndarray, np.reshape(img, (1, IMAGE_MATRIX_SIZE)), axis=0)\n",
    "        label_ndarray = np.append(label_ndarray, np.reshape(label, (1, NUM_CLASSES)), axis=0)\n",
    "\n",
    "    return image_ndarray, label_ndarray\n",
    "\n",
    "\n",
    "def conv2d(x, W) :\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x) :\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "def weight_variable(shape) :\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape) :\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def deepnn(x) :\n",
    "\n",
    "    with tf.name_scope('reshape') :\n",
    "        x_image = tf.reshape(x, [-1, IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n",
    "\n",
    "    with tf.name_scope('conv1') :\n",
    "        W_conv1 = weight_variable([5, 5, CHANNELS, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    with tf.name_scope('pool1') :\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    with tf.name_scope('conv2') :\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    with tf.name_scope('pool2') :\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "    with tf.name_scope('fc1') :\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    with tf.name_scope('dropout') :\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    with tf.name_scope('fc2') :\n",
    "        W_fc2 = weight_variable([1024, NUM_CLASSES])\n",
    "        b_fc2 = bias_variable([NUM_CLASSES])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    return y_conv, keep_prob\n",
    "class process:\n",
    "    \n",
    "    def main(_):\n",
    "\n",
    "        x = tf.placeholder(tf.float32, [None, IMAGE_MATRIX_SIZE])\n",
    "\n",
    "        y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "        y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv)\n",
    "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        with tf.name_scope('adam_optimizer'):\n",
    "            train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "        with tf.Session() as sess :\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch_step in range(MAX_EPOCH) :\n",
    "                train_data_set = batch_data(TRAIN_DATA_SET, BATCH_SIZE)\n",
    "                train_image, train_label = devide_data_set(train_data_set)\n",
    "\n",
    "                if epoch_step % BATCH_SIZE == 0 :\n",
    "                    train_accuracy = accuracy.eval(feed_dict={x: train_image, y_: train_label, keep_prob: 1.0})\n",
    "                    print('epoch_step %d, training accuracy %g' % (epoch_step, train_accuracy))\n",
    "\n",
    "                train_step.run(feed_dict={x: train_image, y_: train_label, keep_prob: 0.5})\n",
    "            test_image, test_label = devide_data_set(TEST_DATA_SET)\n",
    "            print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "                x: test_image, y_: test_label, keep_prob: 1.0}))\n",
    "\n",
    "def view (p):\n",
    "    import math\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, IMAGE_MATRIX_SIZE])\n",
    "    w0 = tf.Variable(tf.zeros([3]))\n",
    "    w = tf.Variable(tf.zeros([IMAGE_MATRIX_SIZE, 3]))\n",
    "    f = tf.matmul(x, w) + w0\n",
    "    p = tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv)\n",
    "    t = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "    sess = tf.Session()\n",
    "    for epoch_step in range(MAX_EPOCH) :\n",
    "        train_data_set = batch_data(TRAIN_DATA_SET, BATCH_SIZE)\n",
    "        train_image, train_label = devide_data_set(train_data_set)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    p_val = sess.run(p, feed_dict={x:train_image, t:train_label})\n",
    "    fig = plt.figure(figsize = (8,15))\n",
    "    \n",
    "    i=0\n",
    "    for i in range(10):\n",
    "        c = 1\n",
    "        for (image, label, pred) in zip(train_image, train_label, p_val):\n",
    "            prediction, actual = np.argmax(pred), np.argmax(label)\n",
    "            if prediction != i:\n",
    "                continue\n",
    "            if (c < 16  and i == actual) or (c >= 4 and i != actual):\n",
    "                subplot = fig.add_subplot(10,6,i*6+c)\n",
    "                subplot.set_xticks([])\n",
    "                subplot.set_yticks([])\n",
    "                subplot.set_title('%d/%d' % (prediction,actual))\n",
    "                subplot.imshow(image.reshape(28,28,3), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "                c += 1\n",
    "            if c > 24:\n",
    "                break\n",
    "\n",
    "MAX_EPOCH = 100\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "process.main(_)\n",
    "view(process.main)\n",
    "# epoch_step ~, training accuracy ~\n",
    "# test accuracy ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "    \n",
    "x = tf.placeholder(tf.float32, [None, IMAGE_MATRIX_SIZE])\n",
    "w0 = tf.Variable(tf.zeros([3]))\n",
    "w = tf.Variable(tf.zeros([IMAGE_MATRIX_SIZE, 3]))\n",
    "f = tf.matmul(x, w) + w0\n",
    "p = tf.nn.softmax(f)  \n",
    "t = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "sess = tf.Session()\n",
    "for epoch_step in range(MAX_EPOCH) :\n",
    "    train_data_set = batch_data(TRAIN_DATA_SET, BATCH_SIZE)\n",
    "    train_image, train_label = devide_data_set(train_data_set)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "p_val = sess.run(p, feed_dict={x:train_image, t:train_label})\n",
    "fig = plt.figure(figsize = (8,15))\n",
    "for i in range(10):\n",
    "    c = 1\n",
    "    for (image, label, pred) in zip(train_image, train_label, p_val):\n",
    "        prediction, actual = np.argmax(pred), np.argmax(label)\n",
    "        if prediction != i:\n",
    "            continue\n",
    "        if (c < 16  and i == actual) or (c >= 4 and i != actual):\n",
    "            subplot = fig.add_subplot(10,6,i*6+c)\n",
    "            subplot.set_xticks([])\n",
    "            subplot.set_yticks([])\n",
    "            subplot.set_title('%d/%d' % (prediction,actual))\n",
    "            subplot.imshow(image.reshape(28,28,3), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "            c += 1\n",
    "        if c > 24:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_makuwo)",
   "language": "python",
   "name": "conda_makuwo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
